{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Kaggle_TitanicSurvival_Pred.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VishnuPy/MyPython/blob/master/Kaggle_TitanicSurvival_Pred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gEu-eRgyvXF",
        "colab_type": "code",
        "outputId": "ed3188e2-c55c-41c7-db98-312daccbfba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive\n",
        "%ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive\n",
            "\u001b[0m\u001b[01;34m'My Drive'\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcf46fLrzHrl",
        "colab_type": "code",
        "outputId": "53e5190f-8eb2-4ef6-bfec-fa5c325f618c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "with tf.device('/cpu:0'):\n",
        "  random_image_cpu = tf.random_normal((100, 100, 100, 3))\n",
        "  net_cpu = tf.layers.conv2d(random_image_cpu, 32, 7)\n",
        "  net_cpu = tf.reduce_sum(net_cpu)\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  random_image_gpu = tf.random_normal((100, 100, 100, 3))\n",
        "  net_gpu = tf.layers.conv2d(random_image_gpu, 32, 7)\n",
        "  net_gpu = tf.reduce_sum(net_gpu)\n",
        "\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "# Test execution once to detect errors early.\n",
        "try:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "except tf.errors.InvalidArgumentError:\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise\n",
        "\n",
        "def cpu():\n",
        "  sess.run(net_cpu)\n",
        "  \n",
        "def gpu():\n",
        "  sess.run(net_gpu)\n",
        "  \n",
        "# Runs the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f3c36750710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f3c36750710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f3c36750710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f3c36750710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f3c3679a4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f3c3679a4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f3c3679a4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f3c3679a4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "3.3374499179999475\n",
            "GPU (s):\n",
            "0.19241704699970796\n",
            "GPU speedup over CPU: 17x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiJjcY2ryIQI",
        "colab_type": "text"
      },
      "source": [
        "This is my first on Kaggle and hence, in this kernel, I use Kaggle's Getting Started Competition, Titanic: Machine Learning from Disaster, to use my learning of different Aspects of DataScience and ML to beat the odds in coming up with a better Accuracy and low RMSE score.\n",
        "\n",
        "<font color=brown size=5>\n",
        "Data Science Frame Work: \n",
        "<font color=black size=4>\n",
        "    1. Define the Problem: Besides Algorithm, Model and Technology is defined, we need to get the Business Problem defined. Typically this happens with various Stakeholders getting together to articulate it for the Technology to provide sutle requirements like  trade-off between False Negatives and True Negative. These are not available in thie competition. However, we will stick to the Kaggle's evaluation criteria that is to predict the Survival Classifier (1 for Survived and 0 for Not-Survived). Will use Python to Build the Model. \n",
        "    2. Data Collection and Gathering: This would in reality be the task that require lots of efforts and resources. In this, case Kaggle has provided the for downloading (https://www.kaggle.com/c/titanic/data). \n",
        "    But, Data Analysis and Correction w.r.t its integrity, meaning, abberations (Outliers and Missing data) is still to be done and that will be done during Data Clearning activity and thereafter Data Tranformation to make it ready for Machine's consumption. This is usually referred to as Data Wrangling. \n",
        "    3. Exploratory Data Analysis: To understand data in Satistical terms that is Correlations and Linearity between and among Features. Identifying Univariate and Multivariate variables. This can be done either in Pivot and/or Graphical representation. This is where the sutle requirement of Hypothesis and rejection of it with Significance and Confidence will have to be done. \n",
        "    4. Build Model with Data: Preparing the Model to arrive at the rules based on the Data and the Outcome. Data and Expected Outcome will determine the Algorithm to be used. Its not that selecting an Algorithm will be produce the desired output and thruput as it requires the Techiniques and Tricks that are at the Craftman's (call him/her DataScient) disposal. Typically, this forms the Activity of Building and Training the Model. \n",
        "    5. Validate the Model: Validation is the critical step and again Craftmanship comes into play in selecting the Data for Validation(s). This step is significant as it eludicidates if the Model is fit to Predict for Known Data (in ML terminology Overfit) or can work equally good with unseen Data. The opposite of Overfitting is Underfitting and that tells us that the Model is not designed for it to grasp the completeness of the Dataset to understand various possibilities. This is also called Generalized Model. In either of the cases, will have to go back to Previous steps to inculcate the required changes for the Model to have Best fit (Training). \n",
        "    6. Optimize and Strategize: This is task where certain Technical Or Repetative tasks can be given to the Data Engineer and to concerntrate on Optimizing the Model Performance. This is an ongoing tasks as it is expected in real-world that new data keeps coming and required Model to be retrained to maintain the Performance and Prediction Accuracy. \n",
        "<font color=brown size=5>\n",
        "    This kernal starts with Point 2 as first one is already taken care by Kaggle and First part of point 2, of having Raw Data making available for this Competition is also done by Kaggle as well. \n",
        "<font size=4>\n",
        "    We can make use of popular Libraries Python3.x for Data Wrangling. <br>\n",
        "    2.1 Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlHqrb3dyIQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd # for data processing and analysis modeled\n",
        "import matplotlib   # for scientific and visualization\n",
        "import numpy as np  # for scientific computing\n",
        "import scipy as sp  # for scientific computing and mathematics Functions\n",
        "import IPython \n",
        "from IPython import display #  printing of dataframes in Jupyter notebook\n",
        "import sklearn      # for machine learning algorithms\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt\n",
        "%matplotlib inline\n",
        "from matplotlib.pylab import rcParams\n",
        "rcParams['figure.figsize'] = 12, 4\n",
        "\n",
        "#misc libraries\n",
        "import random\n",
        "import time\n",
        "\n",
        "#ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0au25i8yIQO",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4>\n",
        "    2.2 Get to know the Data and go a step further to look at the Individual Charactertics and few more steps further towards gaining knowledge of Dependencies among data parts (Features) of the Data Point (Row)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gnAWnlOyIQP",
        "colab_type": "code",
        "outputId": "2010dc4b-3706-45fd-cae8-b81a1502523d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "data_raw = pd.read_csv('/gdrive/My Drive/MyLearning/MLDLAIPython/Data/TextData/Wine.csv') # this is the data for training and our Evaluation\n",
        "\n",
        "data_val = pd.read_csv('/gdrive/My Drive/MyLearning/MLDLAIPython/Data/TextData/Titanic_test.csv') # this is provided by Kaggle and to be used to Submit the final Predictions\n",
        "\n",
        "# make a copy for future usage to check on data.\n",
        "data_train = data_raw.copy(deep = True)\n",
        "data_test = data_val.copy(deep = True)\n",
        "\n",
        "print (data_train.info())\n",
        "print (\"#\"*50)\n",
        "print (data_test.info())\n",
        "print (\"#\"*50)\n",
        "\n",
        "\"\"\"\n",
        "Combine both Test and Train Datasets for doing analysis on Categorical values (Classes) that may be present \n",
        "only in Test but not in Training Dataset\n",
        "\"\"\"\n",
        "data_combine = [data_train, data_test]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 178 entries, 0 to 177\n",
            "Data columns (total 14 columns):\n",
            "Alcohol                 178 non-null float64\n",
            "Malic_Acid              178 non-null float64\n",
            "Ash                     178 non-null float64\n",
            "Ash_Alcanity            178 non-null float64\n",
            "Magnesium               178 non-null int64\n",
            "Total_Phenols           178 non-null float64\n",
            "Flavanoids              178 non-null float64\n",
            "Nonflavanoid_Phenols    178 non-null float64\n",
            "Proanthocyanins         178 non-null float64\n",
            "Color_Intensity         178 non-null float64\n",
            "Hue                     178 non-null float64\n",
            "OD280                   178 non-null float64\n",
            "Proline                 178 non-null int64\n",
            "Customer_Segment        178 non-null int64\n",
            "dtypes: float64(11), int64(3)\n",
            "memory usage: 19.5 KB\n",
            "None\n",
            "##################################################\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 418 entries, 0 to 417\n",
            "Data columns (total 11 columns):\n",
            "PassengerId    418 non-null int64\n",
            "Pclass         418 non-null int64\n",
            "Name           418 non-null object\n",
            "Sex            418 non-null object\n",
            "Age            332 non-null float64\n",
            "SibSp          418 non-null int64\n",
            "Parch          418 non-null int64\n",
            "Ticket         418 non-null object\n",
            "Fare           417 non-null float64\n",
            "Cabin          91 non-null object\n",
            "Embarked       418 non-null object\n",
            "dtypes: float64(2), int64(4), object(5)\n",
            "memory usage: 36.0+ KB\n",
            "None\n",
            "##################################################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6if7HhnyIQT",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=5>\n",
        "2.2 Data Analysis <nr>\n",
        "<font color=brown size=4>\n",
        "2.2.1 DataTypes: <br>\n",
        "<font color=black size=3>\n",
        "1. There are two continuous quantitative variable namely, Age and Fare. This is in case of both Train and Test datasets.\n",
        "2. There are 5 variable with Object Datatype, meaning, these could be free-flowing Nominal Datatype or Categorical\n",
        "3. There are 5 and 4 Numerical values in Train and Test Datasets respectively. These again could be Ordinal or Nominal. However, the difference between Train and Test is that Survived Variable is not in Test Dataset. This is the Dependent varilable and the rest are potential Independent variables that could be included in the Model for it to come up with the Predictions. \n",
        "    \n",
        "<font color=brown size=4>\n",
        "2.2.2 MissingData: <br>\n",
        "<font color=black size=3>\n",
        "1. In Training Dataset, Age and Cabin Features have missing values. 20% of Age values are missing, where as 80% of Cabin are missing. Will have to retain Age as missing values are less than the Standard prescription that is 40%, moreover, we will have to check Relevance of Age on Survial Chances. Eventhough Cabin has more missing values, relevance (or inference) is to be extracted so as to make a decision. Same with Test Dataset However in Test Dataset, additionally one Fare value missing. This value is to be imputed and probably with relevant Mean value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PobntQUXyIQU",
        "colab_type": "code",
        "outputId": "f96d72b6-4d47-4ef7-fcc1-0d1461cf29cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        }
      },
      "source": [
        "data_train.head(15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic_Acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Ash_Alcanity</th>\n",
              "      <th>Magnesium</th>\n",
              "      <th>Total_Phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid_Phenols</th>\n",
              "      <th>Proanthocyanins</th>\n",
              "      <th>Color_Intensity</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD280</th>\n",
              "      <th>Proline</th>\n",
              "      <th>Customer_Segment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>14.20</td>\n",
              "      <td>1.76</td>\n",
              "      <td>2.45</td>\n",
              "      <td>15.2</td>\n",
              "      <td>112</td>\n",
              "      <td>3.27</td>\n",
              "      <td>3.39</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.97</td>\n",
              "      <td>6.75</td>\n",
              "      <td>1.05</td>\n",
              "      <td>2.85</td>\n",
              "      <td>1450</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>14.39</td>\n",
              "      <td>1.87</td>\n",
              "      <td>2.45</td>\n",
              "      <td>14.6</td>\n",
              "      <td>96</td>\n",
              "      <td>2.50</td>\n",
              "      <td>2.52</td>\n",
              "      <td>0.30</td>\n",
              "      <td>1.98</td>\n",
              "      <td>5.25</td>\n",
              "      <td>1.02</td>\n",
              "      <td>3.58</td>\n",
              "      <td>1290</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>14.06</td>\n",
              "      <td>2.15</td>\n",
              "      <td>2.61</td>\n",
              "      <td>17.6</td>\n",
              "      <td>121</td>\n",
              "      <td>2.60</td>\n",
              "      <td>2.51</td>\n",
              "      <td>0.31</td>\n",
              "      <td>1.25</td>\n",
              "      <td>5.05</td>\n",
              "      <td>1.06</td>\n",
              "      <td>3.58</td>\n",
              "      <td>1295</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14.83</td>\n",
              "      <td>1.64</td>\n",
              "      <td>2.17</td>\n",
              "      <td>14.0</td>\n",
              "      <td>97</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.98</td>\n",
              "      <td>0.29</td>\n",
              "      <td>1.98</td>\n",
              "      <td>5.20</td>\n",
              "      <td>1.08</td>\n",
              "      <td>2.85</td>\n",
              "      <td>1045</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>13.86</td>\n",
              "      <td>1.35</td>\n",
              "      <td>2.27</td>\n",
              "      <td>16.0</td>\n",
              "      <td>98</td>\n",
              "      <td>2.98</td>\n",
              "      <td>3.15</td>\n",
              "      <td>0.22</td>\n",
              "      <td>1.85</td>\n",
              "      <td>7.22</td>\n",
              "      <td>1.01</td>\n",
              "      <td>3.55</td>\n",
              "      <td>1045</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>14.10</td>\n",
              "      <td>2.16</td>\n",
              "      <td>2.30</td>\n",
              "      <td>18.0</td>\n",
              "      <td>105</td>\n",
              "      <td>2.95</td>\n",
              "      <td>3.32</td>\n",
              "      <td>0.22</td>\n",
              "      <td>2.38</td>\n",
              "      <td>5.75</td>\n",
              "      <td>1.25</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1510</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>14.12</td>\n",
              "      <td>1.48</td>\n",
              "      <td>2.32</td>\n",
              "      <td>16.8</td>\n",
              "      <td>95</td>\n",
              "      <td>2.20</td>\n",
              "      <td>2.43</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.57</td>\n",
              "      <td>5.00</td>\n",
              "      <td>1.17</td>\n",
              "      <td>2.82</td>\n",
              "      <td>1280</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13.75</td>\n",
              "      <td>1.73</td>\n",
              "      <td>2.41</td>\n",
              "      <td>16.0</td>\n",
              "      <td>89</td>\n",
              "      <td>2.60</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.29</td>\n",
              "      <td>1.81</td>\n",
              "      <td>5.60</td>\n",
              "      <td>1.15</td>\n",
              "      <td>2.90</td>\n",
              "      <td>1320</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14.75</td>\n",
              "      <td>1.73</td>\n",
              "      <td>2.39</td>\n",
              "      <td>11.4</td>\n",
              "      <td>91</td>\n",
              "      <td>3.10</td>\n",
              "      <td>3.69</td>\n",
              "      <td>0.43</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.40</td>\n",
              "      <td>1.25</td>\n",
              "      <td>2.73</td>\n",
              "      <td>1150</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14.38</td>\n",
              "      <td>1.87</td>\n",
              "      <td>2.38</td>\n",
              "      <td>12.0</td>\n",
              "      <td>102</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.64</td>\n",
              "      <td>0.29</td>\n",
              "      <td>2.96</td>\n",
              "      <td>7.50</td>\n",
              "      <td>1.20</td>\n",
              "      <td>3.00</td>\n",
              "      <td>1547</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Alcohol  Malic_Acid   Ash  ...  OD280  Proline  Customer_Segment\n",
              "0     14.23        1.71  2.43  ...   3.92     1065                 1\n",
              "1     13.20        1.78  2.14  ...   3.40     1050                 1\n",
              "2     13.16        2.36  2.67  ...   3.17     1185                 1\n",
              "3     14.37        1.95  2.50  ...   3.45     1480                 1\n",
              "4     13.24        2.59  2.87  ...   2.93      735                 1\n",
              "5     14.20        1.76  2.45  ...   2.85     1450                 1\n",
              "6     14.39        1.87  2.45  ...   3.58     1290                 1\n",
              "7     14.06        2.15  2.61  ...   3.58     1295                 1\n",
              "8     14.83        1.64  2.17  ...   2.85     1045                 1\n",
              "9     13.86        1.35  2.27  ...   3.55     1045                 1\n",
              "10    14.10        2.16  2.30  ...   3.17     1510                 1\n",
              "11    14.12        1.48  2.32  ...   2.82     1280                 1\n",
              "12    13.75        1.73  2.41  ...   2.90     1320                 1\n",
              "13    14.75        1.73  2.39  ...   2.73     1150                 1\n",
              "14    14.38        1.87  2.38  ...   3.00     1547                 1\n",
              "\n",
              "[15 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw9RuysXyIQX",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4>\n",
        "2.2.3 Individual Feature Analysis <br>\n",
        "<font size=4>\n",
        "2.2.3 Categorical values: Going by the above\n",
        "<font color=black size=3>\n",
        "    > Survived Feature is already numeric and its Categorical, 1 Denotes Survived and 0 denotes otherwise.  <br>\n",
        "    > Pclass -- is Ordinal and denotes 1=Upper, 2=Middle 3=Lower and already in numeric format  <br>\n",
        "    > Name is a Nominal type and as it is is of no use in the Predictor Model.<br>\n",
        "    > Sex is categorical and in nominal format. Hence, this should be convered to numeric type.  <br>\n",
        "    > SibSp (Sibling & Spouse) and Parch (Parent & Children) are numeric fields and are Ordinal in nature. <br>\n",
        "    > Ticket value is Alpha-numerica and a unique value like PassengerId. These can be dropped.<br>\n",
        "    > Overall Training Sample Numeric Data Distribution after high level analysis: <br>\n",
        "            >> Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).<br>\n",
        "            >> Survived is a categorical feature with 0 or 1 values. <br>\n",
        "            >> Around 38% samples survived representative of the actual survival rate at 32%. <br>\n",
        "            >> Most passengers (> 75%) did not travel with parents or children. <br>\n",
        "            >> Nearly 30% of the passengers had siblings and/or spouse aboard. <br>\n",
        "            >> Fares varied significantly with few passengers (<1%) paying as high as $512. <br>\n",
        "            >> Few elderly passengers (<1%) within age range 65-80. <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgCQHXKzyIQY",
        "colab_type": "code",
        "outputId": "b10f0bbd-c001-4cee-92f0-2ddb9304f97b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#data_train.describe(include=['O'])\n",
        "data_train.describe"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.describe of      Alcohol  Malic_Acid   Ash  ...  OD280  Proline  Customer_Segment\n",
              "0      14.23        1.71  2.43  ...   3.92     1065                 1\n",
              "1      13.20        1.78  2.14  ...   3.40     1050                 1\n",
              "2      13.16        2.36  2.67  ...   3.17     1185                 1\n",
              "3      14.37        1.95  2.50  ...   3.45     1480                 1\n",
              "4      13.24        2.59  2.87  ...   2.93      735                 1\n",
              "5      14.20        1.76  2.45  ...   2.85     1450                 1\n",
              "6      14.39        1.87  2.45  ...   3.58     1290                 1\n",
              "7      14.06        2.15  2.61  ...   3.58     1295                 1\n",
              "8      14.83        1.64  2.17  ...   2.85     1045                 1\n",
              "9      13.86        1.35  2.27  ...   3.55     1045                 1\n",
              "10     14.10        2.16  2.30  ...   3.17     1510                 1\n",
              "11     14.12        1.48  2.32  ...   2.82     1280                 1\n",
              "12     13.75        1.73  2.41  ...   2.90     1320                 1\n",
              "13     14.75        1.73  2.39  ...   2.73     1150                 1\n",
              "14     14.38        1.87  2.38  ...   3.00     1547                 1\n",
              "15     13.63        1.81  2.70  ...   2.88     1310                 1\n",
              "16     14.30        1.92  2.72  ...   2.65     1280                 1\n",
              "17     13.83        1.57  2.62  ...   2.57     1130                 1\n",
              "18     14.19        1.59  2.48  ...   2.82     1680                 1\n",
              "19     13.64        3.10  2.56  ...   3.36      845                 1\n",
              "20     14.06        1.63  2.28  ...   3.71      780                 1\n",
              "21     12.93        3.80  2.65  ...   3.52      770                 1\n",
              "22     13.71        1.86  2.36  ...   4.00     1035                 1\n",
              "23     12.85        1.60  2.52  ...   3.63     1015                 1\n",
              "24     13.50        1.81  2.61  ...   3.82      845                 1\n",
              "25     13.05        2.05  3.22  ...   3.20      830                 1\n",
              "26     13.39        1.77  2.62  ...   3.22     1195                 1\n",
              "27     13.30        1.72  2.14  ...   2.77     1285                 1\n",
              "28     13.87        1.90  2.80  ...   3.40      915                 1\n",
              "29     14.02        1.68  2.21  ...   3.59     1035                 1\n",
              "..       ...         ...   ...  ...    ...      ...               ...\n",
              "148    13.32        3.24  2.38  ...   1.62      650                 3\n",
              "149    13.08        3.90  2.36  ...   1.33      550                 3\n",
              "150    13.50        3.12  2.62  ...   1.30      500                 3\n",
              "151    12.79        2.67  2.48  ...   1.47      480                 3\n",
              "152    13.11        1.90  2.75  ...   1.33      425                 3\n",
              "153    13.23        3.30  2.28  ...   1.51      675                 3\n",
              "154    12.58        1.29  2.10  ...   1.55      640                 3\n",
              "155    13.17        5.19  2.32  ...   1.48      725                 3\n",
              "156    13.84        4.12  2.38  ...   1.64      480                 3\n",
              "157    12.45        3.03  2.64  ...   1.73      880                 3\n",
              "158    14.34        1.68  2.70  ...   1.96      660                 3\n",
              "159    13.48        1.67  2.64  ...   1.78      620                 3\n",
              "160    12.36        3.83  2.38  ...   1.58      520                 3\n",
              "161    13.69        3.26  2.54  ...   1.82      680                 3\n",
              "162    12.85        3.27  2.58  ...   2.11      570                 3\n",
              "163    12.96        3.45  2.35  ...   1.75      675                 3\n",
              "164    13.78        2.76  2.30  ...   1.68      615                 3\n",
              "165    13.73        4.36  2.26  ...   1.75      520                 3\n",
              "166    13.45        3.70  2.60  ...   1.56      695                 3\n",
              "167    12.82        3.37  2.30  ...   1.75      685                 3\n",
              "168    13.58        2.58  2.69  ...   1.80      750                 3\n",
              "169    13.40        4.60  2.86  ...   1.92      630                 3\n",
              "170    12.20        3.03  2.32  ...   1.83      510                 3\n",
              "171    12.77        2.39  2.28  ...   1.63      470                 3\n",
              "172    14.16        2.51  2.48  ...   1.71      660                 3\n",
              "173    13.71        5.65  2.45  ...   1.74      740                 3\n",
              "174    13.40        3.91  2.48  ...   1.56      750                 3\n",
              "175    13.27        4.28  2.26  ...   1.56      835                 3\n",
              "176    13.17        2.59  2.37  ...   1.62      840                 3\n",
              "177    14.13        4.10  2.74  ...   1.60      560                 3\n",
              "\n",
              "[178 rows x 14 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8obaGKcyIQc",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4>\n",
        "2.3.3 Overall Training Sample set Categorical Data Distribution is: <br>\n",
        "    <font color=black size=3>\n",
        "> Names are unique across the dataset (count=unique=891). <br>\n",
        "> Sex variable as two possible values with 65% male (top=male, freq=577/count=891). <br>\n",
        "> Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin. <br>\n",
        "> Embarked takes three possible values. S port used by most passengers (top=S). <br>\n",
        "> Ticket feature has high ratio (22%) of duplicate values (unique=681). <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YGk7wCryIQd",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size = 4>\n",
        "2.2.4 Assumption based on the Data Analysis done so far <br> \n",
        "<font color=brown size = 3> Correlation:<br> <font color=black> We will have know how each Feature (or the features that would be included in the model) with Survied Feature. Importantly, these are to be done so as to compare with the Modelled Correlation later. <br> \n",
        "<font color=brown size = 3> Completing: <font color=black> <br>\n",
        "    Age Feature data is to be completed as it seem to have strong correlation with Survived Feature. <br>\n",
        "    Embarked feature also have correlation with the Survived Feature and needs to be completed (imputing)\n",
        "<font color=brown size = 3> Correcting: <font color=black> <br>\n",
        "    Ticket Feature has 22% duplicate values and does not contribute much to the Prediction. This field will be dropped. <br><br> \n",
        "    Cabin Feature as it is incomplete and has high number of missing values in both Training and Validation Datasets. <br> <br>\n",
        "    PassengerId will be dropped as this is unique value and certainly have no impact on Survived Feature. <br> <br>\n",
        "    Name Feature also may not have much contribution and will be dropped. \n",
        "<font color=brown size = 3> Creating: <font color=black> <br>\n",
        "    SibSp and Parch are two Features that more or less convey the Familysize. A new feature FamilySize will be created by combinig these two. This new feature gives out number of Family members onboarded. <br> <br>\n",
        "    Name Feature has Title in it which can be used alogn with Sex to establish some kind of correlation. <br><br>\n",
        "    Age and Fare Features are a continous numeric value and will have to create a new Ordinal Categorical Field to bucket them in different ranges. <br> <br>\n",
        "<font color=brown size = 3> Classifying: <font color=black> <br>\n",
        "    Sex Feature to be classified. Women more likely to have survived <br> <br>\n",
        "    Age Feature: Children were more likely to have survived <br> <br>\n",
        "    Pclass Feature: Upper Class (Pclass=1) were more likely to have survived. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdsrq-fMyIQe",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4>\n",
        "    Data Exploration using Pivots and/or Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw5yaEpryIQf",
        "colab_type": "code",
        "outputId": "022284f5-d7e5-4d1a-8d4e-a788ad44e965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "data_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-a43ecd7208a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Pclass'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Survived'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Pclass'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2932\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[0;32m-> 2934\u001b[0;31m                                                    raise_missing=True)\n\u001b[0m\u001b[1;32m   2935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[1;32m   1353\u001b[0m                           raise_missing}\n\u001b[0;32m-> 1354\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[1;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 raise KeyError(\n\u001b[1;32m   1245\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1246\u001b[0;31m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['Pclass', 'Survived'], dtype='object')] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN7_TcvJyIQi",
        "colab_type": "text"
      },
      "source": [
        "Pclass We observe significant correlation (>0.6) among Pclass=1 and Survived (classifying #3). Retain this feature in our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JamWG8vCyIQj",
        "colab_type": "code",
        "outputId": "bbbd912f-9e40-4872-a782-f4bbf5a6d6be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "data_train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-09793f69c39d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Sex\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Survived\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2932\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[0;32m-> 2934\u001b[0;31m                                                    raise_missing=True)\n\u001b[0m\u001b[1;32m   2935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[1;32m   1353\u001b[0m                           raise_missing}\n\u001b[0;32m-> 1354\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[1;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 raise KeyError(\n\u001b[1;32m   1245\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1246\u001b[0;31m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['Sex', 'Survived'], dtype='object')] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1nKgGSQyIQl",
        "colab_type": "text"
      },
      "source": [
        "Assumption is confirmed that Sex=female had very high survival rate at 74% (classifying #1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBBW77eYyIQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHGo-jTjyIQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZcPURbyyIQs",
        "colab_type": "text"
      },
      "source": [
        "SibSp and Parch features have low or zero correlation as the counts go up. It may be best to derive a feature or a set of features from these individual features (creating #1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEE8YLOvyIQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(x = [data_train[data_train['Survived']==1]['Age'], data_train[data_train['Survived']==0]['Age']], \n",
        "         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
        "plt.title('Age Histogram by Survival')\n",
        "plt.xlabel('Age (Years)')\n",
        "plt.ylabel('# of Passengers')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tFTyEgNyIQw",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4>\n",
        "Observations <br>\n",
        "<font color=black size=3>\n",
        "Infants (Age <=4) had high survival rate.<br>\n",
        "Oldest passengers (Age = 80) survived.<br>\n",
        "Large number of 15-25 year olds did not survive.<br>\n",
        "Most passengers are in 15-35 age range. <br>\n",
        "<font color=brown size=4>\n",
        "Decisions<br>\n",
        "<font color=black size=3>\n",
        "This simple analysis confirms our assumptions<br>\n",
        "We should consider Age (confirms our assumption) in our model training.<br>\n",
        "Complete the Age feature for null values. <br>\n",
        "We should create band age groups as New Feature. <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdU04o7AyIQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\n",
        "grid = sns.FacetGrid(data_train, col='Survived', row='Pclass', size=2.2, aspect=1.6)\n",
        "grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n",
        "grid.add_legend();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIQko4_9yIQ1",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4> \n",
        "Observations. <br>\n",
        "<font color=black size=3> \n",
        "Pclass=3 had most passengers, however most did not survive. Confirms assumption.<br>\n",
        "Infant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies assumption. <br>\n",
        "Most passengers in Pclass=1 survived. Confirms assumption. <br>\n",
        "Pclass varies in terms of Age distribution of passengers. <br>\n",
        "\n",
        "<font color=brown size=4> \n",
        "Decisions. <br>\n",
        "<font color=black size=3> \n",
        "Consider Pclass for model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "312fM0LgyIQ2",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4>\n",
        "3.1 Load Data Exploratory Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m1cB92ByIQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grid = sns.FacetGrid(train_df, col='Embarked')\n",
        "grid = sns.FacetGrid(data_train, row='Embarked', height=2.2, aspect=1.6)\n",
        "grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\n",
        "grid.add_legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCiHanqKyIQ7",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4>\n",
        "Observations <br>\n",
        "<font color=black size=3>\n",
        "Female passengers had much better survival rate than males. Confirms classifying assumption.  <br>\n",
        "Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived. <br>\n",
        "Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports.  <br>\n",
        "Ports of embarkation have varying survival rates for Pclass=3 and among male passengers.  <br>\n",
        "\n",
        "<font color=brown size=4>\n",
        "Decisions <br>\n",
        "<font color=black size=3>\n",
        "Add Sex feature to model training.  <br>\n",
        "Complete and add Embarked feature to model training.  <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt74pPI7yIQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid = sns.FacetGrid(data_train, row='Embarked', col='Survived', size=2.2, aspect=1.6)\n",
        "grid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\n",
        "grid.add_legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3DLZzdXyIQ_",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4> \n",
        "Observations <br>\n",
        "<font color=black size=3>\n",
        "Higher fare paying passengers had better survival. Confirms our assumption for creating fare ranges.<br>\n",
        "Port of embarkation correlates with survival rates. Confirms correlating and completing assumptions. <br>\n",
        "\n",
        "<font color=brown size=4> \n",
        "Decisions <br>\n",
        "<font color=black size=3>\n",
        "Consider banding Fare feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrlZJphdyIRA",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4> \n",
        "Wrangle data <br>\n",
        "<font color=black size=3> \n",
        "\n",
        "With confirmed Assumptions and taken decisions, now time to work on Data to Create New Features, Dropping unncessisary Features and Converting Types of features and lastly imputing. \n",
        "\n",
        "<font color=brown size=4> \n",
        "Correcting by dropping features\n",
        "<font color=black size=4> \n",
        "\n",
        "This is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.\n",
        "\n",
        "Based on our assumptions and decisions we want to drop the Cabin (correcting #2) and Ticket (correcting #1) features.\n",
        "\n",
        "Note that where applicable we perform operations on both training and testing datasets together to stay consistent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL8oh1u5yIRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Before\", data_train.shape, data_test.shape, data_combine[0].shape, data_combine[1].shape)\n",
        "\n",
        "data_train = data_train.drop(['Ticket', 'Cabin'], axis=1)\n",
        "data_test = data_test.drop(['Ticket', 'Cabin'], axis=1)\n",
        "data_combine = [data_train, data_test]\n",
        "\n",
        "print(\"After\", data_train.shape, data_test.shape, data_combine[0].shape, data_combine[1].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yvOr5spyIRD",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4> \n",
        "Creating new feature extracting from existing <br>\n",
        "    <font color=black size=3> \n",
        "We want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features.\n",
        "\n",
        "In the following code we extract Title feature using regular expressions. The RegEx pattern (\\w+\\.) matches the first word which ends with a dot character within Name feature. The expand=False flag returns a DataFrame.<br>\n",
        "<font color=brown size=4> \n",
        "Observations<br>\n",
        "<font color=black size=3> \n",
        "When we plot Title, Age, and Survived, we note the following observations.\n",
        "\n",
        "Most titles band Age groups accurately. For example: Master title has Age mean of 5 years.\n",
        "Survival among Title Age bands varies slightly.\n",
        "Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer).<br>\n",
        "\n",
        "<font color=brown size=4> \n",
        "Decision <br>\n",
        "<font color=black size=3> \n",
        "We decide to retain the new Title feature for model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqO4tuG0yIRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for dataset in data_combine:\n",
        "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "\n",
        "pd.crosstab(data_train['Title'], data_train['Sex'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j00uOu4DyIRH",
        "colab_type": "text"
      },
      "source": [
        "We can replace many titles with a more common name or classify them as Rare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTiNH9E_yIRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for dataset in data_combine:\n",
        "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n",
        " \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
        "\n",
        "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
        "    \n",
        "data_train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jPrAGYnyIRL",
        "colab_type": "text"
      },
      "source": [
        "We can convert the categorical titles to ordinal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QzUljCpyIRN",
        "colab_type": "text"
      },
      "source": [
        "Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rvQSXIByIRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
        "for dataset in data_combine:\n",
        "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
        "    dataset['Title'] = dataset['Title'].fillna(0)\n",
        "\n",
        "data_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTLur2JdyIRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train = data_train.drop(['Name', 'PassengerId'], axis=1)\n",
        "data_test = data_test.drop(['Name'], axis=1)\n",
        "data_combine = [data_train, data_test]\n",
        "data_train.shape, data_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S4hLhf9yIRU",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4>\n",
        "Converting a categorical feature <br>\n",
        "<font color=black size=3>\n",
        "Now we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.\n",
        "\n",
        "Let us start by converting Sex feature to a new feature called Gender where female=1 and male=0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi2i2c39yIRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for dataset in data_combine:\n",
        "    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n",
        "\n",
        "data_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbwXmguTyIRZ",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4>\n",
        "Completing a numerical continuous feature <br>\n",
        "    <font color=black size=3>\n",
        "Now we should start estimating and completing features with missing or null values. We will first do this for the Age feature.\n",
        "\n",
        "We can consider three methods to complete a numerical continuous feature.\n",
        "\n",
        "1. A simple way is to generate random numbers between mean and standard deviation.\n",
        "\n",
        "2. More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using median values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...\n",
        "\n",
        "3. Combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\n",
        "\n",
        "Method 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3Kgu9AdyIRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')\n",
        "grid = sns.FacetGrid(data_train, row='Pclass', col='Sex', size=2.2, aspect=1.6)\n",
        "grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n",
        "grid.add_legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbRV2r73yIRe",
        "colab_type": "text"
      },
      "source": [
        "Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bj8bnnGyIRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "guess_ages = np.zeros((2,3))\n",
        "guess_ages"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXYtSTG3yIRi",
        "colab_type": "text"
      },
      "source": [
        "Now we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrG4SSr3yIRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for dataset in data_combine:\n",
        "    for i in range(0, 2):\n",
        "        for j in range(0, 3):\n",
        "            guess_df = dataset[(dataset['Sex'] == i) & \\\n",
        "                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n",
        "            age_guess = guess_df.median()\n",
        "            # Convert random age float to nearest .5 age\n",
        "            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n",
        "            \n",
        "    for i in range(0, 2):\n",
        "        for j in range(0, 3):\n",
        "            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n",
        "                    'Age'] = guess_ages[i,j]\n",
        "\n",
        "    dataset['Age'] = dataset['Age'].astype(int)\n",
        "\n",
        "data_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXxf2hzxyIRl",
        "colab_type": "text"
      },
      "source": [
        "Let us create Age bands and determine correlations with Survived."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhf-vgpUyIRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train['AgeBand'] = pd.cut(data_train['Age'], 5)\n",
        "data_train[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ib-5mcDyIRq",
        "colab_type": "text"
      },
      "source": [
        "Let us replace Age with ordinals based on these bands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJprqZZEyIRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for dataset in data_combine:    \n",
        "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n",
        "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
        "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
        "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
        "    dataset.loc[ dataset['Age'] > 64, 'Age']\n",
        "data_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEQ19VHKyIRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#AgeBand feature can be removed\n",
        "data_train = data_train.drop(['AgeBand'], axis=1)\n",
        "data_combine = [data_train, data_test]\n",
        "data_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IRmvAy3yIR1",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4>\n",
        "    Creating a new Feature: <br>\n",
        "    <font color=black size=3>\n",
        "    New FamilySize by addining up Sibsp and Parch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMXpPAn7yIR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for dataset in data_combine:\n",
        "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
        "\n",
        "data_train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYk5bj_uyIR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# another new feature called IsAlone. \n",
        "\n",
        "for dataset in data_combine:\n",
        "    dataset['IsAlone'] = 0\n",
        "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
        "\n",
        "data_train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ1DPLx4yIR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with IsAlone field with good correlation with Survived Feature, can drop Parch, Sibsp and Familysize Features\n",
        "data_train = data_train.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n",
        "data_test = data_test.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n",
        "data_combine = [data_train, data_test]\n",
        "\n",
        "data_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVZGUiOUyIR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Another New feature combining Pclass and Age.\n",
        "for dataset in data_combine:\n",
        "    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n",
        "\n",
        "data_train.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr7JhF9syIR-",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4>\n",
        "    Completing a categorical feature <br>\n",
        "    <font color=black size=3>\n",
        "Embarked feature takes S, Q, C values based on port of embarcation. Our training dataset has two missing values. We simply fill these with the most common occurance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P41yNVNzyIR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "freq_port = data_train.Embarked.dropna().mode()[0]\n",
        "\n",
        "for dataset in data_combine:\n",
        "    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n",
        "    \n",
        "data_train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKtzmhWlyISC",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4>\n",
        "    Converting categorical feature to numeric <br>\n",
        "<font color=black size=3>\n",
        "We can now convert the Embarked feature by creating a new numeric Port feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BHsGhYpyISD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for dataset in data_combine:\n",
        "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
        "\n",
        "data_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xki68Y_UyISF",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4> \n",
        "Quick completing and converting a numeric feature <br>\n",
        "<font color=black size=3>\n",
        "We can now complete the Fare feature for single missing value in test dataset using mode to get the value that occurs most frequently for this feature. We do this in a single line of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jv_2WIqyISF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_test['Fare'].fillna(data_test['Fare'].dropna().median(), inplace=True)\n",
        "data_test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCsYHNYpyISI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fare has continous numeric data and hence to be converted to category range to make it categorical\n",
        "data_train['FareBand'] = pd.qcut(data_train['Fare'], 4)\n",
        "data_train[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl0Ep9tNyISK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the Fare feature to ordinal values based on the FareBand\n",
        "for dataset in data_combine:\n",
        "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
        "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
        "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
        "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n",
        "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
        "\n",
        "data_train = data_train.drop(['FareBand'], axis=1)\n",
        "data_combine = [data_train, data_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt50P8psyISM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (data_train.head(10))\n",
        "print (\"#\"*75)\n",
        "print (data_test.head(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3eN_wNpyISP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "def correlation_heatmap(df):\n",
        "    _ , ax = plt.subplots(figsize =(8, 12))\n",
        "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
        "    \n",
        "    _ = sns.heatmap(\n",
        "        data_train.corr(), \n",
        "        cmap = colormap,\n",
        "        square=True, \n",
        "        cbar_kws={'shrink':.6 }, \n",
        "        ax=ax,\n",
        "        annot=True, \n",
        "        linewidths=0.1,vmax=1.0, linecolor='white',\n",
        "        annot_kws={'fontsize':12 }\n",
        "    )\n",
        "    \n",
        "    plt.title('Pearson Correlation of Features', y=1.05, size=12)\n",
        "\n",
        "correlation_heatmap(data_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeyeYQkByIST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#set correlation above 0.75 and see true/false values\n",
        "abs(data_train.corr())> 0.50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7gob7nNyISW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.heatmap(data_train.corr(), center=0);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei1N2hiZyISY",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4>\n",
        "4.1 Load Data Modelling Libraries <br>\n",
        "<font color=black size=3>\n",
        "There are many Predictive Modelling Algorithms. However, below are narrowed down give the given problem of Supervised Learning (as dataset is being used for Training the Model) and the Classification Prediction (if a passenger is survived or not).<br>\n",
        "1. Logistic Regression\n",
        "2. KNN or k-Nearest Neighbors\n",
        "3. Support Vector Machines (SVM) and LinearSVM\n",
        "4. Naive Bayes classifier\n",
        "5. Decision Tree\n",
        "6. Random Forrest and Gradient Descents\n",
        "7. Perceptron\n",
        "8. XGB\n",
        "9. CatBoost\n",
        "10. Voting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QecZx24QyISZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = data_train.drop(\"Survived\", axis=1)\n",
        "Y_train = data_train[\"Survived\"]\n",
        "X_test  = data_test.drop(\"PassengerId\", axis=1).copy() #comebackhear\n",
        "X_train.shape, Y_train.shape, X_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK8iVsKVyISb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# machine learning\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn import metrics   #Additional scklearn functions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGCDB2MMyISg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, Y_train)\n",
        "\n",
        "Y_pred = logreg.predict(X_test)\n",
        "acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n",
        "print ('Test ACC Logistic Regression -- > ', acc_log)\n",
        "\n",
        "# generating ROC and RMSC for training just to get hang of if RMSE is going down or up with each model\n",
        "X_pred = logreg.predict(X_train)\n",
        "X_predprob = logreg.predict_proba(X_train)[:,1]\n",
        "t_lr_score = metrics.accuracy_score(Y_train, X_pred)\n",
        "print ('Score -- >', t_lr_score)\n",
        "t_lr_roc = metrics.roc_auc_score(Y_train, X_predprob)\n",
        "print ('ROC -- > ',  t_lr_roc)\n",
        "t_lr_rmse = metrics.mean_squared_error(Y_train, X_predprob)\n",
        "print ('RMSC -- > ',  t_lr_rmse)\n",
        "#X_train, Y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcjY6dpKyISt",
        "colab_type": "text"
      },
      "source": [
        "With Logistic Regression we can validate Assumption and Decisions made for Creating and Completing Feature Goals. Internally, Algorithm calculates the coefficients of the features in decision function. \n",
        "Positve Coefficients increase the Odds of probability of right Prediction and Negative Coefs decrease the Odds. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtE8FRD7yISu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coeff_df = pd.DataFrame(data_train.columns.delete(0))\n",
        "coeff_df.columns = ['Feature']\n",
        "coeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n",
        "\n",
        "coeff_df.sort_values(by='Correlation', ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLKDiYl8yISw",
        "colab_type": "text"
      },
      "source": [
        "In this model, \n",
        "> Sex has highest Positive Correlation/Coefficient implying as the value of Sex increases (from 0=Male to 1=Female) the probability of Survived=1 increases. So, is the Title has the second highest Postive Correlation. \n",
        "\n",
        "> Same with Pclass but it has inverse relationship, that is, Pclass=1 to 3 increases, Surival=1 decreases. This way Age*Class Artificial feature is the second best negative correlation with Survived. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qwXs1HuyISx",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=5> \n",
        "    SVM (Support Vector Machines) <br>\n",
        "<font color=black size=3> \n",
        "SVM is a non-probabilistic Binary Classifier. This  is a Supervised Learning model with associated Learning Algorithms that analyze data for Classification and Regression. Given training samples, each sample will be marks/assigns to one of the two categories and thus makes it Non-Probabilistic. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcbbfNO7yISx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Support Vector Machines\n",
        "\n",
        "svc = SVC(probability=True)  # DO NOT FORGET TO ADD probability=True PARAMETER FOR CAPTURING metrics\n",
        "svc.fit(X_train, Y_train)\n",
        "Y_pred = svc.predict(X_test)\n",
        "acc_svc = round(svc.score(X_train, Y_train) * 100, 2)\n",
        "\n",
        "print ('Test ACC SVM -- > ', acc_svc)\n",
        "# generating ROC and RMSC for training just to get hang of if RMSE is going down or up with each model\n",
        "X_pred = svc.predict(X_train)\n",
        "X_predprob = svc.predict_proba(X_train)[:,1]\n",
        "t_svm_score = metrics.accuracy_score(Y_train, X_pred)\n",
        "print ('Score -- >', t_svm_score)\n",
        "t_svm_roc = metrics.roc_auc_score(Y_train, X_predprob)\n",
        "print ('ROC -- > ',  t_svm_roc)\n",
        "t_svm_rmse = metrics.mean_squared_error(Y_train, X_predprob)\n",
        "print ('RMSC -- > ',  t_svm_rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV3eGJr_4x3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV \n",
        "  \n",
        "# defining parameter range \n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
        "              'kernel': ['linear', 'rbf']}  \n",
        "  \n",
        "#grid = GridSearchCV(LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr'), param_grid, refit = True, verbose = 2) \n",
        "grid = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr')\n",
        "grid.fit(X_train, Y_train) \n",
        "# print best parameter after tuning \n",
        "#print(grid.get_params) \n",
        "  \n",
        "# print how our model looks after hyper-parameter tuning \n",
        "#print(grid.param_grid) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbTxBopu5vgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# fitting the model for grid search \n",
        "svc1 = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
        "                           decision_function_shape='ovr', degree=3,\n",
        "                           gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
        "                           probability=True, random_state=None, shrinking=True,\n",
        "                           tol=0.001, verbose=False)\n",
        "\n",
        "#svc1 = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr')\n",
        "\n",
        "svc1.fit(X_train, Y_train) \n",
        "Y_pred = svc1.predict(X_test)\n",
        "acc_svc1 = round(svc1.score(X_train, Y_train) * 100, 2)\n",
        "\n",
        "print ('Test ACC SVC 1 -- > ', acc_svc)\n",
        "# generating ROC and RMSC for training just to get hang of if RMSE is going down or up with each model\n",
        "X_pred = svc1.predict(X_train)\n",
        "X_predprob = svc1.predict_proba(X_train)[:,1]\n",
        "t_svm1_score = metrics.accuracy_score(Y_train, X_pred)\n",
        "print ('Score -- >', t_svm1_score)\n",
        "t_svm1_roc = metrics.roc_auc_score(Y_train, X_predprob)\n",
        "print ('ROC -- > ',  t_svm1_roc)\n",
        "t_svm1_rmse = metrics.mean_squared_error(Y_train, X_predprob)\n",
        "print ('RMSC -- > ',  t_svm1_rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXhdkzlkyIS0",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=5> \n",
        "    KNN (K-Nearest Neighbours) <br>\n",
        "<font color=black size=3> \n",
        "KNN is a non-parametric method used for Classification (and Regression). Classification happens with the majority of votes it gets from its neighbours, more votes and the Prediction is assigned to that Classifier Class. K is a positive integer, typically small (K=1). When K is 1, then the object is assigned to the class that of that single nearest neighbour. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJACFkjNyIS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "knn = KNeighborsClassifier(n_neighbors = 3)\n",
        "knn.fit(X_train, Y_train)\n",
        "Y_pred = knn.predict(X_test)\n",
        "acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n",
        "print ('Test ACC KNN ', acc_knn)\n",
        "\n",
        "X_pred = knn.predict(X_train)\n",
        "X_predprob = knn.predict_proba(X_train)[:,1]\n",
        "t_knn_score = metrics.accuracy_score(Y_train, X_pred)\n",
        "print ('Score -- >', t_knn_score)\n",
        "t_knn_roc = metrics.roc_auc_score(Y_train, X_predprob)\n",
        "print ('ROC -- > ',  t_knn_roc)\n",
        "t_knn_rmse = metrics.mean_squared_error(Y_train, X_predprob)\n",
        "print ('RMSC -- > ',  t_knn_rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwgPXL6IyIS2",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=5> \n",
        "    Naive Bayes <br>\n",
        "<font color=black size=3> \n",
        "    Naive Bayes classifiers are the family of simple probabilistic classifiers based on applying Bayes Theorm with strong assumption of Independence assumption between the Features. This Algorithm is highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. \n",
        "    Drawback of this is that its sheldom is the case in real-time to have such Independent Features. With such correlation between the feature in our case, it most probably will have the low confidence levels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7hie6u0yIS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gaussian Naive Bayes\n",
        "\n",
        "gaussian = GaussianNB()\n",
        "gaussian.fit(X_train, Y_train)\n",
        "Y_pred = gaussian.predict(X_test)\n",
        "acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\n",
        "\n",
        "\n",
        "print ('Test ACC Naive ', acc_gaussian)\n",
        "\n",
        "X_pred = gaussian.predict(X_train)\n",
        "X_predprob = gaussian.predict_proba(X_train)[:,1]\n",
        "t_nb_score = metrics.accuracy_score(Y_train, X_pred)\n",
        "print ('Score -- >', t_nb_score)\n",
        "t_nb_roc = metrics.roc_auc_score(Y_train, X_predprob)\n",
        "print ('ROC -- > ',  t_nb_roc)\n",
        "t_nb_rmse = metrics.mean_squared_error(Y_train, X_predprob)\n",
        "print ('RMSC -- > ',  t_nb_rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYeDBJZryIS5",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=5> \n",
        "    Perceptron <br>\n",
        "<font color=black size=3> \n",
        "    Perceptron is a supervised learning of Binary classifiers with functions that decide whether an input, represented by a vector of numbers,belongs to a specific class. This is typically Linear classifier, that is prediction happens based on a linear predictor function with addition of having weights assigned to feature vector. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix9YxqXdyIS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perceptron\n",
        "\n",
        "perceptron = Perceptron(penalty='l2')\n",
        "perceptron.fit(X_train, Y_train)\n",
        "Y_pred = perceptron.predict(X_test)\n",
        "acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\n",
        "\n",
        "print ('Test ACC Naive ', acc_perceptron)\n",
        "\"\"\"\n",
        "X_pred = perceptron.predict(X_train)\n",
        "X_predprob = perceptron.predict_proba(X_train)[:,1]\n",
        "t_perc_score = metrics.accuracy_score(Y_train, X_pred)\n",
        "print ('Score -- >', t_perc_score)\n",
        "t_perc_roc = metrics.roc_auc_score(Y_train, X_predprob)\n",
        "print ('ROC -- > ',  t_perc_roc)\n",
        "t_perc_rmse = metrics.mean_squared_error(Y_train, X_predprob)\n",
        "print ('RMSC -- > ',  t_perc_rmse)\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAQMbuYTyIS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Linear SVC\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "\n",
        "linear_svc = LinearSVC() # kernel='linear',probability=True\n",
        "#linear_svc = SklearnClassifier(SVC(kernel='linear',probability=True))\n",
        "linear_svc.fit(X_train, Y_train)\n",
        "Y_pred = linear_svc.predict(X_test)\n",
        "acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n",
        "\n",
        "print ('Test ACC Naive ', acc_linear_svc)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDayoslryIS_",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=5>\n",
        "    Gradient Descent <br>\n",
        "<font color=black size=3>\n",
        "Every Machine Learning Engineer is looking to improve their model performance. Gradient Descent one of the most popular optimization algorithm that helps machine learning models converge at a minimum value through repeated steps. Essentially, gradient descent is used to minimize a function by finding the value that gives the lowest output of that function. Often times, this function is usually a loss function. Loss functions measure how bad our model performs compared to actual occurrences. Hence, it only makes sense that we should reduce this loss. One way to do this is via Gradient Descent. This works on the same principle of Linear Relationships between Independent and Dependent variables. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI5RCIvOyITA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gradient Descent\n",
        "\n",
        "gd = GradientBoostingClassifier()\n",
        "gd.fit(X_train, Y_train)\n",
        "Y_pred = gd.predict(X_test)\n",
        "acc_gd = round(gd.score(X_train, Y_train) * 100, 2)\n",
        "\n",
        "print ('Test ACC Gradient Descent', acc_gd)\n",
        "\n",
        "X_pred = gd.predict(X_train)\n",
        "X_predprob = gd.predict_proba(X_train)[:,1]\n",
        "t_gd_score = metrics.accuracy_score(Y_train, X_pred)\n",
        "print ('Score -- >', t_gd_score)\n",
        "t_gd_roc = metrics.roc_auc_score(Y_train, X_predprob)\n",
        "print ('ROC -- > ',  t_gd_roc)\n",
        "t_gd_rmse = metrics.mean_squared_error(Y_train, X_predprob)\n",
        "print ('RMSC -- > ',  t_gd_rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uB2-a0syITE",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=5>\n",
        "    Stochastic Gradient Descent <br>\n",
        "<font color=black size=3>  \n",
        "In Gradient Descent Algorithm, gradients on each observation is done one by one. It becomes resource intensive when the Dataset is Large. To overcome this, Observations are Randomly picked up. This random probabilistic selection of Observations makes this Stochastic. This Algorithm offers wide variety of parameters to minimize the lost, increase the scope and pace of descent (learning). We will explore these options later to improve on the performance. Let us for now use the basic algorithm with defaults. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdpG2HY9yITF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stochastic Gradient Descent\n",
        "\n",
        "sgd = SGDClassifier (loss='log') #()\n",
        "sgd.fit(X_train, Y_train)\n",
        "Y_pred = sgd.predict(X_test)\n",
        "acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n",
        "\n",
        "print ('Test ACC Stochastic Gradient Descent', acc_sgd)\n",
        "X_pred = sgd.predict(X_train)\n",
        "X_predprob = sgd.predict_proba(X_train)[:,1]\n",
        "t_sgd_score = metrics.accuracy_score(Y_train, X_pred)\n",
        "print ('Score -- >', t_sgd_score)\n",
        "t_sgd_roc = metrics.roc_auc_score(Y_train, X_predprob)\n",
        "print ('ROC -- > ',  t_sgd_roc)\n",
        "t_sgd_rmse = metrics.mean_squared_error(Y_train, X_predprob)\n",
        "print ('RMSC -- > ',  t_sgd_rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwjBTS6tyITH",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=5>\n",
        "    Random Forest\n",
        "<font color=black size=3>\n",
        "    Random Forest are Ensemble learning method for classification and regression. This model Operates by constructing multitude of Decision Trees at training time and the output is the mode of classes (Classification) or mean prediction (regression) of individual Trees. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8MQxYumyITH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random Forest\n",
        "\n",
        "random_forest = RandomForestClassifier(n_estimators=100)\n",
        "random_forest.fit(X_train, Y_train)\n",
        "Y_pred = random_forest.predict(X_test)\n",
        "random_forest.score(X_train, Y_train)\n",
        "acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n",
        "acc_random_forest\n",
        "\n",
        "print ('Test ACC Random Forest', acc_random_forest)\n",
        "X_pred = random_forest.predict(X_train)\n",
        "X_predprob = random_forest.predict_proba(X_train)[:,1]\n",
        "t_rm_score = metrics.accuracy_score(Y_train, X_pred)\n",
        "print ('Score -- >', t_rm_score)\n",
        "t_rm_roc = metrics.roc_auc_score(Y_train, X_predprob)\n",
        "print ('ROC -- > ',  t_rm_roc)\n",
        "t_rm_rmse = metrics.mean_squared_error(Y_train, X_predprob)\n",
        "print ('RMSC -- > ',  t_rm_rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae0fUIF-yITM",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=5>\n",
        "   XGBoost (Extreme Gradient Boosting)\n",
        "<font color=black size=3>\n",
        "    XGBoost is a decision tree-based Ensemble machine learning algorithm that uses Gradient Boosting framework. In prediction problems involving unstructured data (images, text etc) artificial neural networks tend to outperform other algorithm frameworks. However, when it comes to small-to-medium structure/tabular data, decision tree based algorithms are considered best-in class now. \n",
        "    \n",
        "Evaluation of Ensemble models started with Decision trees, a graphical representation of possible solutions to a decision based on certain conditions.<br> \n",
        "    > <font color=brown size=4> Bootstraping aggregating or Bagging <font color=black size=3> ensemble meta-algorithms combining predictions from multiple decision trees through a voting mechaism, gave rise to Boosting Algorithms. <br>\n",
        "    > Baggin-based algorithms where only <font color=brown size=4> sub-set of features are selected at random to build a forest <font color=black size=3> or collection of decision trees, gave rise to Random Forest Algorithm <br>\n",
        "    > Models are <font color=brown size=4> build sequentially by minimizing errors <font color=black size=3> from previous models while increasing the influence of high-performing models, gave rise to Boosting Algorithms. <br>\n",
        "    > On these Boosting Algorithms, when additionally <font color=brown size=4> employed Gradient Descent algorithm <font color=black size=3> to minimize errors, gave rise to Gradient Descent Algorithms. <br>\n",
        "    > <font color=brown size=4> Optimized Gradient Boosting Algorithm <font color=black size=3> by employing parallel processing , tree purning, handling missing values and regularization to avoid overfitting (or Bias), gave rise to this new Queen of Machine Learning Algorithms. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXiRywVeyITN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb = XGBClassifier(n_estimators=100)\n",
        "xgb.fit(X_train, Y_train)\n",
        "Y_pred_xgb=xgb.predict(X_test)\n",
        "xgb.score(X_train, Y_train)\n",
        "acc_xgb = round(xgb.score(X_train, Y_train) * 100, 2)\n",
        "\n",
        "\n",
        "print ('Test ACC XGBoost', acc_xgb)\n",
        "X_pred = xgb.predict(X_train)\n",
        "X_predprob = xgb.predict_proba(X_train)[:,1]\n",
        "t_xgb_score = metrics.accuracy_score(Y_train, X_pred)\n",
        "print ('Score -- >', t_xgb_score)\n",
        "t_xgb_roc = metrics.roc_auc_score(Y_train, X_predprob)\n",
        "print ('ROC -- > ',  t_xgb_roc)\n",
        "t_xgb_rmse = metrics.mean_squared_error(Y_train, X_predprob)\n",
        "print ('RMSC -- > ',  t_xgb_rmse)\n",
        "\n",
        "#Parameters list can be found here as well: https://xgboost.readthedocs.io/en/latest/parameter.html\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owcfJRwCyITO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## have tunned parameters using GridSearch and randomly to come up with better score\n",
        "## refer to this file this file\n",
        "xgb_tuned = XGBClassifier(\n",
        " learning_rate =0.1,\n",
        " n_estimators=143,\n",
        " max_depth=5,\n",
        " min_child_weight=1,\n",
        " gamma=0.0,\n",
        " subsample=0.8,\n",
        " colsample_bytree=0.8,\n",
        " objective= 'binary:logistic',\n",
        " nthread=4,\n",
        " scale_pos_weight=1,\n",
        " seed=27\n",
        ")\n",
        "xgb_tuned.fit(X_train, Y_train)\n",
        "Y_pred_tuned=xgb_tuned.predict(X_test)\n",
        "xgb_tuned.score(X_train, Y_train)\n",
        "acc_xgb_tuned = round(xgb_tuned.score(X_train, Y_train) * 100, 2)\n",
        "acc_xgb_tuned\n",
        "\n",
        "print ('Test ACC XGBoost Tunned', acc_xgb_tuned)\n",
        "X_pred = xgb_tuned.predict(X_train)\n",
        "X_predprob = xgb_tuned.predict_proba(X_train)[:,1]\n",
        "t_xgbt_score = metrics.accuracy_score(Y_train, X_pred)\n",
        "print ('Score -- >', t_xgbt_score)\n",
        "t_xgbt_roc = metrics.roc_auc_score(Y_train, X_predprob)\n",
        "print ('ROC -- > ',  t_xgbt_roc)\n",
        "t_xgbt_rmse = metrics.mean_squared_error(Y_train, X_predprob)\n",
        "print ('RMSC -- > ',  t_xgbt_rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkciIZOMyITQ",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=5>\n",
        "    Decision Trees\n",
        "<font color=black size=3>\n",
        "    Decision Tree is a predictive model that maps features (Tree Branches) to conclusions about the target value (Tree Leaves). In this model, target variable takes a finite set of values called Classification trees; in these tree structures, leaves represent class labels and braches represent conjuctions of features that lead to those class labels. Decision Trees where target variable can take continous values, typically real numbers, are called Regression Trees. \n",
        "    \n",
        "Drawback of this Learning Model is that they tend to Overfit and are rigid due to the defined Tree Structured formed during Training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcZJ3WI0yITR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decision Tree\n",
        "\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "decision_tree.fit(X_train, Y_train)\n",
        "Y_pred = decision_tree.predict(X_test)\n",
        "acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n",
        "\n",
        "print ('Test ACC Decision Tree', acc_decision_tree)\n",
        "X_pred = decision_tree.predict(X_train)\n",
        "X_predprob = decision_tree.predict_proba(X_train)[:,1]\n",
        "t_dt_score = metrics.accuracy_score(Y_train, X_pred)\n",
        "print ('Score -- >', t_dt_score)\n",
        "t_dt_roc = metrics.roc_auc_score(Y_train, X_predprob)\n",
        "print ('ROC -- > ',  t_dt_roc)\n",
        "t_dt_rmse = metrics.mean_squared_error(Y_train, X_predprob)\n",
        "print ('RMSC -- > ',  t_dt_rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKP7izYDyITS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "bag_cla = BaggingClassifier()\n",
        "bag_cla.fit(X_train, Y_train)\n",
        "\n",
        "y_pred=bag_cla.predict(X_test)\n",
        "acc_bag_cla = round(bag_cla.score(X_train, Y_train) * 100, 2)\n",
        "\n",
        "print ('Test ACC Decision Tree', acc_bag_cla)\n",
        "X_pred = bag_cla.predict(X_train)\n",
        "X_predprob = bag_cla.predict_proba(X_train)[:,1]\n",
        "t_bc_score = metrics.accuracy_score(Y_train, X_pred)\n",
        "print ('Score -- >', t_bc_score)\n",
        "t_bc_roc = metrics.roc_auc_score(Y_train, X_predprob)\n",
        "print ('ROC -- > ',  t_bc_roc)\n",
        "t_bc_rmse = metrics.mean_squared_error(Y_train, X_predprob)\n",
        "print ('RMSC -- > ',  t_bc_rmse)\n",
        "# Summary of the predictions made by the classifier\n",
        "#print(classification_report(test1_y_dummy,y_pred))\n",
        "#print(confusion_matrix(y_pred,test1_y_dummy))\n",
        "\n",
        "#Accuracy Score\n",
        "#print('accuracy is ',accuracy_score(y_pred,test1_y_dummy))\n",
        "\n",
        "#BCC = accuracy_score(y_pred,test1_y_dummy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cvnz1zzyITk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XGBt = XGBClassifier(base_score=0.5, booster='gbtree',\n",
        "                                      colsample_bylevel=1, colsample_bynode=1,\n",
        "                                      colsample_bytree=0.8, gamma=0,\n",
        "                                      learning_rate=0.1, max_delta_step=0,\n",
        "                                      max_depth=5, min_child_weight=1,\n",
        "                                      missing=None, n_estimators=140, n_jobs=1,\n",
        "                                      nthread=4, objective='binary:logistic',\n",
        "                                      random_state=0, reg_alpha=0, reg_lambda=1,\n",
        "                                      scale_pos_weight=1, seed=27, silent=None,\n",
        "                                      subsample=0.8, verbosity=1)\n",
        "\n",
        "XGBt.fit(X_train, Y_train)\n",
        "\n",
        "y_pred=XGBt.predict(X_test)\n",
        "acc_XGBt = round(XGBt.score(X_train, Y_train) * 100, 2)\n",
        "\n",
        "print ('Test ACC XGBoost Tunned', acc_XGBt)\n",
        "X_pred = XGBt.predict(X_train)\n",
        "X_predprob = XGBt.predict_proba(X_train)[:,1]\n",
        "t_XGBt_score = metrics.accuracy_score(Y_train, X_pred)\n",
        "print ('Score -- >', t_XGBt_score)\n",
        "t_XGBt_roc = metrics.roc_auc_score(Y_train, X_predprob)\n",
        "print ('ROC -- > ',  t_XGBt_roc)\n",
        "t_XGBt_rmse = metrics.mean_squared_error(Y_train, X_predprob)\n",
        "print ('RMSC -- > ',  t_XGBt_rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVPVY08P8IQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uND7XYKNnlW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://catboost.ai/docs/concepts/python-reference_parameters-list.html\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "catb=CatBoostClassifier(iterations=2500, depth=5, learning_rate=0.3, verbose=0, \n",
        "                        allow_writing_files=False, train_dir='/gdrive/My Drive/MyLearning/MLDLAIPython/Data/TextData/'\n",
        "                        , loss_function='CrossEntropy', random_strength=0.1, leaf_estimation_method='Gradient') \n",
        "catb.fit(X_train, Y_train)\n",
        "\n",
        "y_pred=catb.predict(X_test)\n",
        "acc_catb = round(catb.score(X_train, Y_train) * 100, 2)\n",
        "\n",
        "print ('Test ACC CatBoost', acc_catb)\n",
        "X_pred = catb.predict(X_train)\n",
        "X_predprob = catb.predict_proba(X_train)[:,1]\n",
        "t_catb_score = metrics.accuracy_score(Y_train, X_pred)\n",
        "print ('Score -- >', t_catb_score)\n",
        "t_catb_roc = metrics.roc_auc_score(Y_train, X_predprob)\n",
        "print ('ROC -- > ',  t_catb_roc)\n",
        "t_catb_rmse = metrics.mean_squared_error(Y_train, X_predprob)\n",
        "print ('RMSC -- > ',  t_catb_rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZGCU0F8Os2U",
        "colab_type": "text"
      },
      "source": [
        "Depth=5; Learning rate=0.35; Test ACC CatBoost 86.53\n",
        "Score -- > 0.8653198653198653\n",
        "ROC -- >  0.9277261155316952\n",
        "RMSC -- >  0.09759353137609376\n",
        "\n",
        "Learning rate=0.4; Test ACC CatBoost 86.53\n",
        "Score -- > 0.8653198653198653\n",
        "ROC -- >  0.9298671694415152\n",
        "RMSC -- >  0.09661935963862305\n",
        "\n",
        "Learning rate=0.5; Test ACC CatBoost 86.64\n",
        "Score -- > 0.8664421997755332\n",
        "ROC -- >  0.9307992202729044\n",
        "RMSC -- >  0.09632914936388477\n",
        "\n",
        "Depth=6; Learning rate=0.6; Test ACC CatBoost 86.64\n",
        "Score -- > 0.8664421997755332\n",
        "ROC -- >  0.9315288829237635\n",
        "RMSC -- >  0.09603915812168021\n",
        "\n",
        "Learning rate=0.7; Test ACC CatBoost 86.64\n",
        "Score -- > 0.8664421997755332\n",
        "ROC -- >  0.9310016084534347\n",
        "RMSC -- >  0.09597660934122997\n",
        "\n",
        "Learning rate=0.1;LeafEstMethod='Gradient' Test ACC CatBoost 86.53\n",
        "Score -- > 0.8653198653198653\n",
        "ROC -- >  0.928146869907008\n",
        "RMSC -- >  0.09794536278520466\n",
        "\n",
        "Learning rate=0.3; Test ACC CatBoost 86.64\n",
        "Score -- > 0.8664421997755332\n",
        "ROC -- >  0.9307779162539014\n",
        "RMSC -- >  0.09633228736150076"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vE5gQaknzD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "catb.get_all_params()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtYReyPkEly3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "clr = LogisticRegression()\n",
        "csvc = SVC(probability=True) \n",
        "cknn = KNeighborsClassifier(n_neighbors = 3)\n",
        "cgau = GaussianNB()\n",
        "cgb = GradientBoostingClassifier()\n",
        "csgb = SGDClassifier (loss='log')\n",
        "crf = RandomForestClassifier(n_estimators=100)\n",
        "cxgbt = XGBClassifier(learning_rate =0.1, n_estimators=143, max_depth=5, min_child_weight=1, gamma=0.0, subsample=0.8, colsample_bytree=0.8,\n",
        "                     objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
        "cdt = DecisionTreeClassifier()\n",
        "cbc = BaggingClassifier()\n",
        "ccb = CatBoostClassifier(iterations=2500, depth=5, learning_rate=0.3, verbose=0, \n",
        "                        allow_writing_files=False #, train_dir='/gdrive/My Drive/MyLearning/MLDLAIPython/Data/TextData/'\n",
        "                        , loss_function='CrossEntropy', random_strength=0.1, leaf_estimation_method='Gradient') \n",
        "\"\"\"\n",
        "\n",
        "Model\tScore\tROC\tRMSE\n",
        "9\tDecision Tree\t86.76\t0.936802\t0.092976\n",
        "3\tRandom Forest\t86.76\t0.932711\t0.095217\n",
        "13\tCat Boost\t86.64\t0.930778\t0.096332\n",
        "12\tBagging Classifer\t86.42\t0.930269\t0.097128\n",
        "14\tVoting Classifier\t85.30\t0.924664\t0.104306\n",
        "11\tXgBoost_Tuned\t85.86\t0.921660\t0.102103\n",
        "\"\"\"\n",
        "ccb = CatBoostClassifier(iterations=2500, depth=5, learning_rate=0.3, verbose=0, \n",
        "                        allow_writing_files=False, train_dir='/gdrive/My Drive/MyLearning/MLDLAIPython/Data/TextData/'\n",
        "                        , loss_function='CrossEntropy', random_strength=0.1, leaf_estimation_method='Gradient') \n",
        "#eclf1 = VotingClassifier(estimators=[('LogReg', clr), ('SVC', csvc), ('KNN', cknn), ('GradientBoost', cgb), ('StochaisticGB', csgb), \n",
        "#                                     ('RandomForest', crf), ('XGB', cxgb), ('DecisionTree', cdt), ('BaggClassifier', cbc), ('CatBoost', ccb)], voting='soft')\n",
        "\n",
        "eclf1 = VotingClassifier(estimators=[('RandomForest', crf), ('DecisionTree', cdt), ('BaggClassifier', cbc), ('CatBoost', ccb)], \n",
        "                         voting='soft')\n",
        "\n",
        "eclf1.fit(X_train, Y_train)\n",
        "y_pred=catb.predict(X_test)\n",
        "acc_eclf1 = round(eclf1.score(X_train.astype(float), Y_train.astype(float)).astype(float) * 100, 2)\n",
        "#acc_eclf1 = eclf1.score(X_train.astype('float64'), Y_train.astype('float64'))\n",
        "\n",
        "print ('Test ACC CatBoost', acc_eclf1)\n",
        "X_pred = eclf1.predict(X_train.astype(float))\n",
        "X_predprob = eclf1.predict_proba(X_train)[:,1]\n",
        "t_eclf1_score = metrics.accuracy_score(Y_train, X_pred)\n",
        "print ('Score -- >', t_eclf1_score)\n",
        "t_eclf1_roc = metrics.roc_auc_score(Y_train, X_predprob)\n",
        "print ('ROC -- > ',  t_eclf1_roc)\n",
        "t_eclf1_rmse = metrics.mean_squared_error(Y_train, X_predprob)\n",
        "print ('RMSC -- > ',  t_eclf1_rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKLxGzKfErO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "for clf, label in zip([crf, cdt, cbc, ccb, eclf1], ['RandomForest',  'Decision Tree', 'Bagging Classifier', 'CatBoost', 'Voting Classifier']\n",
        "                      ):\n",
        "  scores = cross_val_score(clf, X_train, Y_train, cv=5, scoring='accuracy')\n",
        "  print (\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsl0xOSRyITU",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=6>\n",
        "Model Evaluation\n",
        "<font color=black size=3>\n",
        "Lets rank the evaluation of all the models to choose the best. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhiu8TGgyITV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = pd.DataFrame({\n",
        "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
        "              'Random Forest', 'Naive Bayes', 'Perceptron', 'Gradient Descent',\n",
        "              'Stochastic Gradient Descent', 'Linear SVC', \n",
        "              'Decision Tree', 'XgBoost', 'XgBoost_Tuned', 'Bagging Classifer', 'Cat Boost', 'Voting Classifier'],\n",
        "    'Score': [acc_svc, acc_knn, acc_log, \n",
        "              acc_random_forest, acc_gaussian, acc_perceptron, \n",
        "              acc_gd,acc_sgd, acc_linear_svc, acc_decision_tree, acc_xgb,acc_xgb_tuned, acc_bag_cla, acc_catb, \n",
        "              acc_eclf1],\n",
        "    'ROC': [t_svm_roc, t_knn_roc, t_lr_roc, \n",
        "              t_rm_roc, t_nb_roc, -100, \n",
        "              t_gd_roc,t_sgd_roc, -100, t_dt_roc, t_xgb_roc,t_xgbt_roc, t_bc_roc, t_catb_roc, t_eclf1_roc],\n",
        "    'RMSE': [t_svm_rmse, t_knn_rmse, t_lr_rmse, \n",
        "              t_rm_rmse, t_nb_rmse, 100, \n",
        "              t_gd_rmse,t_sgd_rmse, 100, t_dt_rmse, t_xgb_rmse,t_xgbt_rmse, t_bc_rmse, t_catb_rmse, t_eclf1_rmse]\n",
        "    \n",
        "})\n",
        "models.sort_values(by='ROC', ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5PCwCcuyITX",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4>\n",
        "As can see, Random Forest and Decision Trees have the same scores. But Decision Tree beats Random Forest with low error rate, RMSE. Hence, will be submitting this. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFE3UmLmyITZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "submission = pd.DataFrame({\n",
        "        \"PassengerId\": data_test[\"PassengerId\"],\n",
        "        \"Survived\": Y_pred\n",
        "    })\n",
        "\n",
        "submission.to_csv(\"submission_TitanicSurvived_pred.csv\", index=False)\n",
        "\n",
        "print('Validation Data Distribution: \\n', submission['Survived'].value_counts(normalize = True))\n",
        "submission.sample(10)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7TQyxoUxD5r",
        "colab_type": "text"
      },
      "source": [
        "<font color=brown size=4>\n",
        "As can see, Random Forest and Decision Trees have the same scores but Decision Tree Beats Random Forest in RMSE. The evaluation is based on lowest RMSE and hence, will be using it for submission on 02Oct19. \n",
        "Earlier Submissions:\n",
        "    -- A week ago, Random forest but now, when i checked RMSE score, choosing DT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrw8Jgf2yITh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_pred = decision_tree.predict(X_test)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "        \"PassengerId\": data_test[\"PassengerId\"],\n",
        "        \"Survived\": Y_pred\n",
        "    })\n",
        "\n",
        "print (\"Not Normalized Counts\")\n",
        "print (submission.Survived.value_counts())\n",
        "\n",
        "\n",
        "print('Validation Data Distribution NORMalized: \\n', submission['Survived'].value_counts(normalize = True))\n",
        "print (submission.sample(10))\n",
        "\n",
        "submission.to_csv(\"/gdrive/My Drive/MyLearning/MLDLAIPython/Data/TextData/submission_TitanicSurvived_pred_02Oct19_1500hr.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyAvk-tLtH__",
        "colab_type": "text"
      },
      "source": [
        "<font color=darkblue size=10>\n",
        "new code new code new code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXBHvi98xOwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decision Tree Max Depth Parameter tunning\n",
        "\n",
        "max_depths = np.linspace(1, 32, 32, endpoint=True)\n",
        "train_results = []\n",
        "test_results = []\n",
        "for max_depth in max_depths:\n",
        "   dt = DecisionTreeClassifier(max_depth=max_depth)\n",
        "   dt.fit(X_train, Y_train)\n",
        "   train_pred = dt.predict(X_train)\n",
        "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
        "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
        "   # Add auc score to previous train results\n",
        "   train_results.append(roc_auc)\n",
        "   y_pred = dt.predict(X_test)\n",
        "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
        "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
        "   # Add auc score to previous test results\n",
        "   test_results.append(roc_auc)\n",
        "\n",
        "line1, = plt.plot(max_depths, train_results, 'b', label='Train AUC')\n",
        "line2, = plt.plot(max_depths, test_results, 'r', label='Test AUC')\n",
        "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
        "plt.ylabel('AUC score')\n",
        "plt.xlabel('Tree depth')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXf1upcDrRZx",
        "colab_type": "text"
      },
      "source": [
        "Test ACC Decision Tree 86.76\n",
        "Score -- > 0.867564534231201\n",
        "ROC -- >  0.9368016276270519\n",
        "RMSC -- >  0.09297596703891177"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_T_NX4hrJll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}